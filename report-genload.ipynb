{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":951996,"sourceType":"datasetVersion","datasetId":516716}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import (\n    AutoFeatureExtractor, \n    AutoTokenizer, \n    VisionEncoderDecoderModel,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer, \n    default_data_collator,\n)\n\nfrom torch.utils.data import Dataset\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom pathlib import Path\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:03:02.042295Z","iopub.execute_input":"2024-04-12T12:03:02.042967Z","iopub.status.idle":"2024-04-12T12:03:21.158264Z","shell.execute_reply.started":"2024-04-12T12:03:02.042938Z","shell.execute_reply":"2024-04-12T12:03:21.157239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_projections.csv')\ndf1 = pd.read_csv('/kaggle/input/chest-xrays-indiana-university/indiana_reports.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:03:21.159750Z","iopub.execute_input":"2024-04-12T12:03:21.160318Z","iopub.status.idle":"2024-04-12T12:03:21.244919Z","shell.execute_reply.started":"2024-04-12T12:03:21.160292Z","shell.execute_reply":"2024-04-12T12:03:21.244135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_captions_df = pd.DataFrame({'imgs': [],\n                                    'captions': []})\nfor i in range(len(df2)):\n    uid = df2.iloc[i]['uid']\n    image = df2.iloc[i]['filename']\n    index = df1.loc[df1['uid'] ==uid]\n    \n    if not index.empty:    \n        index = index.index[0]\n        caption = df1.iloc[index]['findings']\n        if type(caption) == float:\n         \n            continue \n        images_captions_df = pd.concat([images_captions_df, pd.DataFrame([{'imgs': image, 'captions': caption}])], ignore_index=True)\nimages_captions_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:03:24.621625Z","iopub.execute_input":"2024-04-12T12:03:24.622010Z","iopub.status.idle":"2024-04-12T12:03:31.998277Z","shell.execute_reply.started":"2024-04-12T12:03:24.621980Z","shell.execute_reply":"2024-04-12T12:03:31.997334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_checkpoint = \"google/vit-base-patch16-224-in21k\"\ndecoder_checkpoint = \"Molkaatb/ChestX\"\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(encoder_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:04:13.990862Z","iopub.execute_input":"2024-04-12T12:04:13.991294Z","iopub.status.idle":"2024-04-12T12:04:15.527994Z","shell.execute_reply.started":"2024-04-12T12:04:13.991262Z","shell.execute_reply":"2024-04-12T12:04:15.527136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = '/kaggle/input/chest-xrays-indiana-university/images/images_normalized/'\nimages_captions_df['imgs'] = p+ images_captions_df['imgs']\nimages_captions_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:04:26.237656Z","iopub.execute_input":"2024-04-12T12:04:26.237990Z","iopub.status.idle":"2024-04-12T12:04:26.249541Z","shell.execute_reply.started":"2024-04-12T12:04:26.237967Z","shell.execute_reply":"2024-04-12T12:04:26.248598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# maximum length for the captions\nmax_length = 1024\nsample = images_captions_df.iloc[99]\n\n# sample image\nimage = Image.open(sample['imgs']).convert('RGB')\n# sample caption\ncaption = sample['captions']\n\n# apply feature extractor on the sample image\ninputs = feature_extractor(images=image, return_tensors='pt')\n# apply tokenizer\noutputs = tokenizer(\n            caption, \n            max_length=max_length, \n            \n            padding='max_length',\n            return_tensors='pt',\n        )\nprint(len(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:04:36.941239Z","iopub.execute_input":"2024-04-12T12:04:36.941594Z","iopub.status.idle":"2024-04-12T12:04:37.191746Z","shell.execute_reply.started":"2024-04-12T12:04:36.941569Z","shell.execute_reply":"2024-04-12T12:04:37.190759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LoadDataset(Dataset):\n    def __init__(self, df):\n        self.images = images_captions_df['imgs'].values\n        self.captions = images_captions_df['captions'].values\n        \n    def __getitem__(self, idx):\n        # everything to return is stored inside this dict\n        inputs = dict()\n\n        # load the image and apply feature_extractor\n        image_path = str(self.images[idx])\n        image = Image.open(image_path).convert(\"RGB\")\n        image = feature_extractor(images=image, return_tensors='pt')\n\n        # load the caption and apply tokenizer\n        caption = self.captions[idx]\n        labels = tokenizer(\n            caption, \n            max_length=max_length, \n            truncation=True, \n            padding='max_length',\n            return_tensors='pt',\n        )['input_ids'][0]\n        \n        # store the inputs and labels in the dict we created\n        inputs['pixel_values'] = image['pixel_values'].squeeze()   \n        inputs['labels'] = labels\n        return inputs\n    \n    def __len__(self):\n        return len(self.images)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:04:39.201244Z","iopub.execute_input":"2024-04-12T12:04:39.201938Z","iopub.status.idle":"2024-04-12T12:04:39.210082Z","shell.execute_reply.started":"2024-04-12T12:04:39.201908Z","shell.execute_reply":"2024-04-12T12:04:39.209036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_,test_df =train_test_split(images_captions_df, test_size=0.10, shuffle=True, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:04:41.851888Z","iopub.execute_input":"2024-04-12T12:04:41.852298Z","iopub.status.idle":"2024-04-12T12:04:41.863581Z","shell.execute_reply.started":"2024-04-12T12:04:41.852267Z","shell.execute_reply":"2024-04-12T12:04:41.862236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df,val_df =train_test_split(train_, test_size=0.10, shuffle=True, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:04:44.048636Z","iopub.execute_input":"2024-04-12T12:04:44.049003Z","iopub.status.idle":"2024-04-12T12:04:44.056482Z","shell.execute_reply.started":"2024-04-12T12:04:44.048973Z","shell.execute_reply":"2024-04-12T12:04:44.055626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_df))\nprint(len(val_df))\nprint(len(test_df))","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:04:46.571027Z","iopub.execute_input":"2024-04-12T12:04:46.571702Z","iopub.status.idle":"2024-04-12T12:04:46.576915Z","shell.execute_reply.started":"2024-04-12T12:04:46.571671Z","shell.execute_reply":"2024-04-12T12:04:46.575908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = LoadDataset(train_df)\ntest_ds = LoadDataset(test_df)\nval_ds = LoadDataset(val_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:04:49.150173Z","iopub.execute_input":"2024-04-12T12:04:49.150528Z","iopub.status.idle":"2024-04-12T12:04:49.155135Z","shell.execute_reply.started":"2024-04-12T12:04:49.150501Z","shell.execute_reply":"2024-04-12T12:04:49.154122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:04:51.035256Z","iopub.execute_input":"2024-04-12T12:04:51.035887Z","iopub.status.idle":"2024-04-12T12:04:51.045371Z","shell.execute_reply.started":"2024-04-12T12:04:51.035855Z","shell.execute_reply":"2024-04-12T12:04:51.044406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_pretrained(\"Molkaatb/ChestX\").to('cuda')\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n# model.config.vocab_size = model.config.decoder.vocab_size\nmodel.config.num_beams = 4","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:04:58.443009Z","iopub.execute_input":"2024-04-12T12:04:58.444188Z","iopub.status.idle":"2024-04-12T12:05:24.582313Z","shell.execute_reply.started":"2024-04-12T12:04:58.444144Z","shell.execute_reply":"2024-04-12T12:05:24.580962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm \npredicted_captions = [] \nfor i in tqdm.tqdm( val_df['imgs']):\n    img =  Image.open(i).convert(\"RGB\")\n    features = feature_extractor(img, return_tensors=\"pt\").pixel_values.to(\"cuda\")\n    caption = tokenizer.decode(model.generate(features,max_length = 1024)[0],skip_special_tokens=True)\n    predicted_captions.append(caption)\nprint(len(predicted_captions))","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:05:29.343215Z","iopub.execute_input":"2024-04-12T12:05:29.343593Z","iopub.status.idle":"2024-04-12T12:16:22.448053Z","shell.execute_reply.started":"2024-04-12T12:05:29.343562Z","shell.execute_reply":"2024-04-12T12:16:22.446814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n\n# Assuming you have a list of predicted captions and a list of ground truth captions\ngenerated_captions = predicted_captions\nground_truth_captions = val_df['captions'].values\n# Convert the caption lists into the format expected by nltk\nground_truth_captions = [[caption.split() for caption in captions] for captions in ground_truth_captions]\ngenerated_captions = [caption.split() for caption in generated_captions]\n\n\n# Define the smoothing function to use\nsmoothie = SmoothingFunction().method4\n\n# Compute the BLEU score with smoothing\nweights = (0.25, 0.25, 0.25, 0.25)  # equal weights for 1-4 gram BLEU scores\nscore = corpus_bleu(ground_truth_captions, predicted_captions,weights =weights)\nprint(f'The BELU Score Is: {score}')","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:16:22.450244Z","iopub.execute_input":"2024-04-12T12:16:22.450600Z","iopub.status.idle":"2024-04-12T12:17:12.644376Z","shell.execute_reply.started":"2024-04-12T12:16:22.450568Z","shell.execute_reply":"2024-04-12T12:17:12.643360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2024-04-12T11:59:39.058237Z","iopub.execute_input":"2024-04-12T11:59:39.058947Z","iopub.status.idle":"2024-04-12T11:59:39.062830Z","shell.execute_reply.started":"2024-04-12T11:59:39.058913Z","shell.execute_reply":"2024-04-12T11:59:39.061857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\n\n# Assuming you have a loop \nfor idx in range(40, 50):\n    inputs = val_ds[idx]['pixel_values']\n    with torch.no_grad():\n        # Model prediction \n        out = model.generate(\n            inputs.unsqueeze(0).to('cuda'),  # Move inputs to GPU\n            num_beams=4,\n            max_length=512\n        )\n\n    # Convert token ids to string format\n    decoded_out = tokenizer.decode(out[0], skip_special_tokens=True)\n\n    # Display the result\n    print(f\"Prediction for index {idx}: {decoded_out}\")\n\n    # Display the image\n    plt.figure()\n    plt.axis('off')\n    plt.imshow(torch.permute(inputs, (1, 2, 0)))\n    plt.show()\n    print(\"\\n\\nActual Image and Text\\n\\n\")\n    # Display the text\n    labels_tensor = val_ds[idx]['labels']\n\n    decoded_out = tokenizer.decode(labels_tensor, skip_special_tokens=True)\n\n    print(decoded_out)\n\n    # Display actual image\n    inputs = val_ds[idx]['pixel_values']\n\n    # Convert the PyTorch tensor to a NumPy array\n    image_array = inputs.permute(1, 2, 0).numpy()\n    # Display the image using matplotlib\n    plt.figure()\n    plt.axis('off')\n    plt.imshow(image_array)\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T12:17:20.521604Z","iopub.execute_input":"2024-04-12T12:17:20.522428Z","iopub.status.idle":"2024-04-12T12:17:36.501895Z","shell.execute_reply.started":"2024-04-12T12:17:20.522393Z","shell.execute_reply":"2024-04-12T12:17:36.500684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}